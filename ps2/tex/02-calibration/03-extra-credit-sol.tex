\begin{answer}\\
Adding $L_2$ regularization changes our cost function\\
$J(\theta)=\frac{1}{m}\sum_{i=1}^{m}-[y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))] + \lambda||\theta||^2$\\
now $\frac{\partial}{\partial \theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}x_j^{(i)}(h_{\theta}(x^{(i)})-y^{(i)})+2 \lambda \theta_j$\\
Therefore adding $L_2$ regularization skews the intercept by a $constant \times \theta_i$\\
For the bias term we have\\
$\frac{\partial}{\partial \theta_0}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}x_0^{(i)}(h_{\theta}(x^{(i)})-y^{(i)})+2 \lambda \theta_0$\\
Setting this to zero we get\\
$\frac{1}{m}\sum_{i=1}^{m}y^{(i)}=\frac{1}{m}\sum_{i=1}^{m}h_{\theta}(x^{(i)}) + 2\lambda \theta_0$\\
By adding regularization we shift our prediction by $2\lambda \theta_0$\\
Note that usually $L_2$ regularization is usually not applied to the bias term to account for this.
\end{answer}
